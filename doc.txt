gcloud compute disks describe {{ disk_name }}
        --zone {{ zone }}
--> check if the disk exists (result.rc != 0)

gcloud compute instances create vm2 \
        --zone europe-west1-c \
        --machine-type n1-standard-1 \
        --network-interface subnet=default \
        --network-interface subnet=sub30

gcloud compute instances describe vm1 \
        --zone europe-west1-c \
        --format="value(status)"
--> stdout.find('RUNNING') == -1 to check if vm is not running ("stdout": "RUNNING")

gcloud compute instances describe vm2 \
        --zone=europe-west1-c \
        --format="value(networkInterfaces[0].accessConfigs[0].natIP)"

gcloud compute disks list

gcloud compute routes create openvpnnet \
    --destination-range=172.16.252.0/24 \
    --network=reginavpc1 \
    --next-hop-instance=vpnserver

===========================================================================================
gcloud auth configure-docker
export PROJECT_ID=gns3-235219
gcloud projects describe gns3-235219

docker images
gcloud config set project $PROJECT_ID
gcloud config set compute/zone us-west1-a

docker tag netboxcommunity/netbox:latest gcr.io/gns3-235219/netbox
gcloud docker push gcr.io/gns3-235219/netbox
docker push gcr.io/gns3-235219/netbox
gcloud container images list --repository=gcr.io/gns3-235219


gcloud container clusters create tier-1-cluster \
    --project service-project-1-id \
    --zone=us-central1-a \
    --enable-ip-alias \
    --network projects/host-project-id/global/networks/shared-net \
    --subnetwork projects/host-project-id/regions/us-central1/subnetworks/tier-1 \
    --cluster-secondary-range-name tier-1-pods \
    --services-secondary-range-name tier-1-services
    
gcloud container clusters create cl1 --subnetwork default
NAME  LOCATION    MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION    NUM_NODES  STATUS
cl1   us-west1-a  1.14.10-gke.17  104.198.12.236  n1-standard-1  1.14.10-gke.17  3          RUNNING

gcloud compute instances list
gke-cl1-default-pool-663e07c3-001n  us-west1-a      n1-standard-1               10.138.0.3               35.247.65.46    RUNNING
gke-cl1-default-pool-663e07c3-5jjl  us-west1-a      n1-standard-1               10.138.0.4               35.233.183.237  RUNNING
gke-cl1-default-pool-663e07c3-fb1g  us-west1-a      n1-standard-1               10.138.0.5               35.230.36.116   RUNNING

kubectl create deployment hello-web --image=gcr.io/google-samples/hello-app:1.0
kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
hello-web-5677597687-qkqqm   1/1     Running   0          8s

kubectl expose deployment hello-web --type=LoadBalancer --port 80 --target-port 8080
kubectl get service hello-web
NAME        TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE
hello-web   LoadBalancer   10.35.255.173   34.82.146.149   80:32423/TCP   104s

curl http://34.82.146.149:80
Hello, world!
Version: 1.0.0
Hostname: hello-web-5677597687-qkqqm

kubectl set image deployment/hello-web hello-app=gcr.io/google-samples/hello-app:2.0

kubectl delete service hello-web
gcloud container clusters delete cl1 --zone us-west1-a



on macbook:
sudo route add -net 10.10.10.0 -netmask 255.255.255.0 -interface utun3
#sudo networksetup -setadditionalroutes utun3 10.10.10.0 255.255.255.0 172.16.252.1
The following command will show the existing routing table (IPv4 only):
netstat -nr -f inet
On EVE NG server:
ip address add 192.168.255.1/24 dev pnet9
echo 1 > /proc/sys/net/ipv4/ip_forward
To generate SSH keys in macOS:
ssh-keygen -t rsa
save this key to the clipboard
pbcopy < ~/.ssh/id_rsa.pub

on vpnserver:
sudo apt-get install nano
sudo nano /etc/sysctl.conf
net.ipv4.ip_forward=1
To update session settings, run:
sudo sysctl -p
sudo systemctl status openvpn@server

sudo apt-get update
sudo apt-get install iputils-ping

duck dns automatic IP address update:
https://www.duckdns.org/update?domains=vpngc&token=8c892437-f280-410c-a7f4-99d8a453a372&ip=

Ansible:
result.rc == 0 or != 0 (return code of the task result)
changed_when: (Overriding The Changed Result)

For example, you could manage your keys using the OS Login feature. To use OS Login:

- Enable OS Login on the instance generated by setting "enable-oslogin" metadata field to "TRUE" via the metadata parameter on GCE instance creation with Ansible.
- Make an IAM user with the roles/iam.serviceAccountUser and roles/compute.osLoginAdmin permissions.
- Either generate a new or choose an existing SSH keypair and upload the public key for use with OS Login: gcloud compute os-login ssh-keys add --key-file [KEY_FILE_PATH] --ttl [EXPIRE_TIME] (where --ttl specifies how long you want this public key to be usable - for example, --ttl 1d will make it expire after 1 day)
- From here, you should be able to configure Ansible to use the service user's name and corresponding private key to the uploaded public key in order to SSH successfully into any OS Login-enabled GCE instances. For example, this can be done by overriding the ansible_user and ansible_ssh_private_key_file inventory parameters, or by passing --private-key and --user parameters to ansible-playbook.
